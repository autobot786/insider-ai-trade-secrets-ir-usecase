{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21a3aeb4",
   "metadata": {},
   "source": [
    "# 04 — Correlation Timeline (Staging → Exfil)\n",
    "\n",
    "This notebook correlates endpoint staging (archive creation), DLP incidents, and proxy uploads to build an investigation timeline.\n",
    "\n",
    "**Data:** `edr.csv`, `dlp.csv`, `proxy.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4246374f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "data_dir = Path('data')\n",
    "edr = pd.read_csv(data_dir/'edr.csv', parse_dates=['timestamp'])\n",
    "dlp = pd.read_csv(data_dir/'dlp.csv', parse_dates=['timestamp'])\n",
    "proxy = pd.read_csv(data_dir/'proxy.csv', parse_dates=['timestamp'])\n",
    "\n",
    "edr.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9de324",
   "metadata": {},
   "source": [
    "## Find archive creation events on endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0819f6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "archive_creates = edr[(edr['user']=='engineer.a') & (edr['event_type']=='file_create') & (edr['file_path'].str.contains(r'\\.(zip|7z|tar)$', na=False))].copy()\n",
    "archive_creates = archive_creates.sort_values('timestamp')\n",
    "archive_creates[['timestamp','device','file_path','file_size','data_sensitivity','process_name']].head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add5f439",
   "metadata": {},
   "source": [
    "## Join to DLP incidents (±2 hours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22970bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "hits = dlp[(dlp['user']=='engineer.a') & (dlp['policy']=='Restricted-AI-IP')].copy().sort_values('timestamp')\n",
    "\n",
    "window = pd.Timedelta('2h')\n",
    "rows=[]\n",
    "for _, a in archive_creates.iterrows():\n",
    "    t=a['timestamp']\n",
    "    cand = hits[(hits['timestamp']>=t-window) & (hits['timestamp']<=t+window)]\n",
    "    for _, e in cand.iterrows():\n",
    "        rows.append({\n",
    "            'archive_time': t,\n",
    "            'archive_path': a['file_path'],\n",
    "            'archive_size': a['file_size'],\n",
    "            'archive_sensitivity': a['data_sensitivity'],\n",
    "            'dlp_time': e['timestamp'],\n",
    "            'dlp_action': e['action'],\n",
    "            'destination': e['destination'],\n",
    "            'file_name': e['file_name'],\n",
    "            'file_size': e['file_size'],\n",
    "        })\n",
    "\n",
    "stage_to_dlp = pd.DataFrame(rows).sort_values('archive_time')\n",
    "stage_to_dlp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d2aedc",
   "metadata": {},
   "source": [
    "## Join to proxy uploads (±2 hours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc7dafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "proxy_personal = proxy[(proxy['user']=='engineer.a') & (proxy['dest_category']=='personal_cloud_storage') & (proxy['http_method'].isin(['POST','PUT']))].copy().sort_values('timestamp')\n",
    "\n",
    "rows=[]\n",
    "for _, a in archive_creates.iterrows():\n",
    "    t=a['timestamp']\n",
    "    cand = proxy_personal[(proxy_personal['timestamp']>=t-window) & (proxy_personal['timestamp']<=t+window)]\n",
    "    for _, p in cand.iterrows():\n",
    "        rows.append({\n",
    "            'archive_time': t,\n",
    "            'archive_path': a['file_path'],\n",
    "            'archive_size': a['file_size'],\n",
    "            'proxy_time': p['timestamp'],\n",
    "            'dest_domain': p['dest_domain'],\n",
    "            'bytes_out': p['bytes_out'],\n",
    "            'process_name': p['process_name'],\n",
    "            'user_agent': p['user_agent']\n",
    "        })\n",
    "\n",
    "stage_to_proxy = pd.DataFrame(rows).sort_values('archive_time')\n",
    "stage_to_proxy.head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc7d0e6",
   "metadata": {},
   "source": [
    "## Build a single timeline view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626ea96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "timeline=[]\n",
    "for _, r in archive_creates.iterrows():\n",
    "    timeline.append({'time': r['timestamp'], 'type': 'archive_created', 'detail': r['file_path'], 'bytes': r['file_size']})\n",
    "for _, r in hits.iterrows():\n",
    "    timeline.append({'time': r['timestamp'], 'type': f\"dlp_{r['action']}\", 'detail': f\"{r['destination']}:{r['file_name']}\", 'bytes': r['file_size']})\n",
    "for _, r in proxy_personal.iterrows():\n",
    "    if r['bytes_out'] > 50_000_000:\n",
    "        timeline.append({'time': r['timestamp'], 'type': 'proxy_upload_personal_cloud', 'detail': f\"{r['dest_domain']} via {r['process_name']}\", 'bytes': r['bytes_out']})\n",
    "\n",
    "t = pd.DataFrame(timeline).sort_values('time')\n",
    "t.head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be90df20",
   "metadata": {},
   "source": [
    "## Investigation notes\n",
    "\n",
    "Use the timeline to:\n",
    "- justify containment decisions\n",
    "- prioritize which devices/repos to image and preserve\n",
    "- create an evidence-backed executive narrative\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
